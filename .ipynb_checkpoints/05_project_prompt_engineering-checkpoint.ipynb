{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f193828c-cca1-422f-a0a0-f27896ca3bd1",
   "metadata": {},
   "source": [
    "# Project : Prompt Engineering\n",
    "\n",
    "\n",
    "The quality of the instructions you give to an LLM can have a large effect on the quality of its outputs, especially for complex tasks. This project related to prompt design will help you learn how to craft prompts that produce accurate and consistent results.\n",
    "\n",
    "The project will be based on : \n",
    "- https://docs.anthropic.com/claude/docs/introduction-to-prompt-design\n",
    "\n",
    "- https://docs.google.com/spreadsheets/d/19jzLgRruG9kjUQNKtCg1ZjdD6l6weA6qRXG5zLIAhC8/edit#gid=150872633\n",
    "\n",
    "However, we will use Cohere and not Claude. It means, that you need to retain the principles of prompt engineering  but not necessarily exactly the same syntax described in the documents\n",
    "\n",
    "Moreover, you can check :\n",
    "- https://python.langchain.com/docs/modules/model_io/prompts/quick_start\n",
    "- https://docs.google.com/presentation/d/1zxkSI7lLUBrZycA-_znwqu8DDyVhHLkQGScvzaZrUns/edit?pli=1#slide=id.g2accb454d71_79_175 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c99f7ec-64c8-415c-a8e9-33c8042bb26d",
   "metadata": {},
   "source": [
    "### Complete pre-requisite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff372e2-dbf4-476c-bee7-09d551d43883",
   "metadata": {},
   "source": [
    "#### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60530a1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv(), override=True)\n",
    "\n",
    "# You can add other package here if needed\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efe4eba-f301-4f50-9260-a7ddb778afac",
   "metadata": {},
   "source": [
    "#### Initialize LLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f36fd57c-ec76-49e6-95ae-368e836725ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mCohere\u001b[0m\n",
      "Params: {'model': None, 'max_tokens': 256, 'temperature': 0.75, 'k': 0, 'p': 1, 'frequency_penalty': 0.0, 'presence_penalty': 0.0, 'truncate': None}\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import Cohere\n",
    "\n",
    "llm = Cohere(temperature=0.75, cohere_api_key=os.environ.get('COHERE_API_KEY'))\n",
    "\n",
    "print(llm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e771f89a-0a87-4b68-9cd4-6c8e481a89ea",
   "metadata": {},
   "source": [
    "## Prompt Design"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1370c0b7",
   "metadata": {},
   "source": [
    "### Being Clear and Direct\n",
    "\n",
    "From : https://docs.google.com/spreadsheets/d/19jzLgRruG9kjUQNKtCg1ZjdD6l6weA6qRXG5zLIAhC8/edit#gid=1733615301\n",
    "\n",
    "\n",
    "**LLM responds best to clear and direct instructions.** \n",
    "\n",
    "Think of llm like any other human that is new to the job. Claude has no context on what to do aside from what you literally tell it. Just as when you instruct a human for the first time on a task, the more you explain exactly what you want in a straightforward manner to LLM, the better and more accurate LLM's response will be.\n",
    "\n",
    "**The Golden Rule of Clear Prompting** : show your prompt to a friend and ask them if they could follow the instructions themselves and produce the exact result you want. If they're confused, any LLM will be confused as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c797b8-9db8-4f1c-8d3d-bf856565b68b",
   "metadata": {},
   "source": [
    "#### Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77638288-af34-4103-9f09-7ca0c7491dd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Robótica, diversión de wireframe,\\ndisfruta del pastel con sabores,\\nfeliz y contento. \\n\\n(Translated) Robotics, wireframe fun,\\nenjoying flavored cake,\\nhappy and content. '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# adapt the prompt to make the llm outpout its answer in Spanish\n",
    "basic_prompt = \"Write a haiku about robots.\"\n",
    "improved_prompt =\"Como poeta experto, escribe un haiku sobre robots. El haiku debe tener la estructura tradicional de 3 versos. Asegúrate de que el haiku transmita una imagen vívida y capture la esencia de los robots en solo unas pocas palabras. Por favor responde únicamente en español.\"\n",
    "llm.invoke(improved_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "902679cb-f1dd-4daf-a584-3066742c7ec0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Michael Jordan '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Modify the prompt so that the llm doesn't equivocate at all and responds with ONLY the name of one specific player, with no other words or punctuation. \n",
    "basic_prompt = \"Who is the best basketball player of all time? Please choose one specific player.\"\n",
    "improved_prompt =\"Respond with only the name of the single best basketball player of all time, in your opinion. Do not include any other words, punctuation, or elaboration in your response - only the name of the player.\"\n",
    "llm.invoke(improved_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aec2110f-677f-4f48-97da-4443515305c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Michael Jordan '"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modify the prompt so that the llm doesn't equivocate at all and responds with ONLY the name of one specific player, with no other words or punctuation. \n",
    "basic_prompt = \"Who is the best basketball player of all time? Please choose one specific player.\"\n",
    "improved_prompt = \"Who is the best basketball player of all time? Respond with only the name of this single player, with no other words or punctuation included in your response.\"\n",
    "llm.invoke(improved_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a537a81-5550-4798-af4d-ac122ca26e36",
   "metadata": {},
   "source": [
    "### Assigning Roles (Role Prompting)\n",
    "\n",
    " From : https://docs.google.com/spreadsheets/d/19jzLgRruG9kjUQNKtCg1ZjdD6l6weA6qRXG5zLIAhC8/edit#gid=2055375080\n",
    " \t\n",
    "Continuing on the theme of LLM having no context aside from what you say, it's sometimes important to prompt llm to inhabit a specific role (including all necessary context). This is also known as role prompting. The more detail to the role context, the better.\n",
    "\n",
    "Priming an llm with a role can improve the performance in a variety of fields, from writing to coding to summarizing. It's like how humans can sometimes be helped when told to \"think like a ______\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1808ae3-6388-460f-b431-55ed46833d3d",
   "metadata": {},
   "source": [
    "#### Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df8f39d6-8638-424e-813d-bde910f3f910",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" A republic is a type of governance in which citizens' elected officials govern a state. Modern republics are based on the principle of elected representation, but this isn't universal, and not every election-based system is a republic. \\n\\nHere are the main characteristics and principles of a republic:\\n\\n1. Representative Government: In a republic, the country is not governed directly by the voters, unlike in a direct democracy. Instead, citizens elect representatives to formulate laws, policies, and decisions on their behalf. Representatives may be elected directly by the people or appointed through a hierarchical system.\\n\\n2. Rule of Law: A republic is governed by law, and its leaders are accountable to the constitution and legal system. The law applies equally to all citizens, including those in power, and it is the responsibility of the government to uphold and protect these laws.\\n\\n3. Sovereignty of the People: In a republican government, the ultimate source of power lies with the people. They have the ability to elect or remove their leaders, amend the constitution, and directly participate in political life. This is often accomplished through democratic practices, such as free and fair elections, freedom of expression and assembly, and the right to petition. \\n\\n4. Separation of Powers: Many\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# generate a better output by giving a role\n",
    "basic_prompt = \"Explains what is a republic.\"\n",
    "improved_prompt = \"As a political science professor, provide a clear and concise explanation of what a republic is. Cover the key characteristics and principles that define a republic, such as representative government, rule of law, and the sovereignty of the people. Aim to explain the concept in a way that is accessible to someone without a strong political background.\"\n",
    "llm.invoke(improved_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2877c34-de6b-4d7a-9e69-89092422c9e3",
   "metadata": {},
   "source": [
    "### Separating Data from Instructions\n",
    "\n",
    "Not able to provide an exercice : https://docs.google.com/spreadsheets/d/19jzLgRruG9kjUQNKtCg1ZjdD6l6weA6qRXG5zLIAhC8/edit#gid=1519813817"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b6171a-0805-4cdd-90bf-85e91b09e9a6",
   "metadata": {},
   "source": [
    "### Formatting Output\n",
    "\n",
    "From: https://docs.google.com/spreadsheets/d/19jzLgRruG9kjUQNKtCg1ZjdD6l6weA6qRXG5zLIAhC8/edit#gid=257656347 \n",
    "\n",
    "An LLM can format its output in a wide variety of ways. You just need to ask for it to do so!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc21042-4c73-4ce1-b797-d8fff0902b71",
   "metadata": {},
   "source": [
    "#### Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85e3342e-2787-4f79-9103-9399b2ede8af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' LeBron James '"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Forced to make a choice, Cohere designates Michael Jordan as the best basketball player of all time. Can we get Cohere to pick someone else?\n",
    "basic_prompt = \"Who is the best basketball player of all time?\"\n",
    "improved_prompt = \"Imagine you are a contrarian sports analyst who likes to challenge conventional wisdom. When asked “Who is the best basketball player of all time?”, instead of picking the common choice of Michael Jordan, you prefer to highlight the accomplishments of a different all-time great player. Respond with only the name of this single player you select, with no other words or punctuation included in your response.\"\n",
    "llm.invoke(improved_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1b0f4824-34ff-4225-a745-96eccaa12bca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Sure, here's an HTML table with the requested summary of significant events in French history:\\n\\n```html\\n<table>\\n    <tr>\\n        <th>Time Period</th>\\n        <th>Event</th>\\n        <th>Significance</th>\\n    </tr>\\n    <tr>\\n        <td>6000 BCE - 2nd Century BCE</td>\\n        <td>\\n            Prehistoric and Gallic Period:\\n            <ul>\\n                <li>Indigenous Celtic tribes inhabit the region that will become France.</li>\\n                <li>Their clans and tribes organize themselves without centralized governance.</li>\\n                <li>Numerous archaeological sites from this period, like Fontéchevade, reveal insights into their culture and tools.</li>\\n            </ul>\\n        </td>\\n        <td>\\n            The foundation of France's rich Celtic heritage and the basis for its language and culture. \\n        </td>\\n    </tr>\\n    <tr>\\n        <td>52 BCE - 27 BCE</td>\\n        <td>\\n            Julius Caesar conducts the Gallic Wars, bringing part of modern-day France under Roman rule.\\n            <br>\\n            <ul>\\n                <li>The Roman Republic establishes colonies, infrastructure, and Roman culture\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Instead of receiving a basic output, you want to ouptut in a table format\n",
    "basic_prompt = \"Gives the main events of France.\"\n",
    "improved_prompt = \"create a table summarizing the main events in the history of France.The table should have the following columns: Time Period: The historical era or date of the event, Event: A brief description of what happened, Significance: The importance or impact of the event on French history, Include at least 10 major events, spanning from ancient times to the modern era. Ensure the events chosen represent the most pivotal moments in France’s political, cultural, and social development.Format the output as an HTML table, with appropriate table headers.\"\n",
    "llm.invoke(improved_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f8eb26-0277-4622-a858-cd3c292d82f7",
   "metadata": {},
   "source": [
    "### Thinking Step by Step \n",
    "\n",
    "From: https://docs.google.com/spreadsheets/d/19jzLgRruG9kjUQNKtCg1ZjdD6l6weA6qRXG5zLIAhC8/edit#gid=1213680236\n",
    "\n",
    " \t\n",
    "If someone woke you up and immediately started asking you several complicated questions that you had to respond to right away, how would you do? Probably not as good as if you were given time to think through your answer first. \n",
    "\n",
    "Guess what? An LLM is the same way.\n",
    "\n",
    "Giving Cohere time to think step by step sometimes makes Cohere more accurate, particularly for complex tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3938c76-4e49-44a6-b68d-24a387c833f2",
   "metadata": {},
   "source": [
    "#### Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f5eec2b-5016-426f-9f5a-1d377cddb284",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' <positive-argument>The term \"great\" indicates that the product is of high quality and satisfaction.</positive-argument>\\n<positive-argument>The reviewer will be recommending this product to friends, which implies a level of satisfaction and willingness to share a positive experience.</positive-argument>\\n<negative-argument>There are no elements in the text indicating a negative sentiment present in the review.</negative-argument>\\nPositive'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Find something where you need to  use the \"Think step by step method\"\n",
    "basic_prompt = \"Is it a negative opinion :This product is great. i'll be recommending it to my friends. ? \"\n",
    "improved_prompt =\"User: Is this review sentiment negative or positive? First write the best arguments for each side in <negative-argument> and <positive-argument> XML tags, then answer. :This product is great. i'll be recommending it to my friends.\"\n",
    "llm.invoke(basic_prompt)\n",
    "llm.invoke(improved_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0508c821-2a96-4fb0-a7eb-e2d39014851b",
   "metadata": {},
   "source": [
    "### Using Examples\n",
    "\n",
    "Check this sheet to understand the difference with \"Formatting output\": https://docs.google.com/spreadsheets/d/19jzLgRruG9kjUQNKtCg1ZjdD6l6weA6qRXG5zLIAhC8/edit#gid=1640903723\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a6fdc2-e790-43e6-ac2f-30ef6a5ad22d",
   "metadata": {},
   "source": [
    "#### Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0649f48b-f151-496c-9f3f-b456c9dca192",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Find something where you need to  use the \"Using Examples\"\n",
    "# The goal is to understanding how it is powerful compared to \"Formatting output\"\n",
    "# When you exactly know what you want\n",
    "basic_prompt = \"\"\n",
    "improved_prompt = \"\"\"You have to help me understand whether the messages my customers leave are positive or negative.\n",
    "Here are some examples of what I expect :\n",
    "Customer: \"This product is great, I'm going to recommend it to other people\"\n",
    "You: Positive\n",
    "Customer: \"This product sucks, it broke in less than 1 week\".\n",
    "You: Negative\n",
    "Customer: \"Great product and great customer service, nothing to add\"\n",
    "You:\"\"\"\n",
    "llm.invoke(improved_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4652da-77b5-48db-b3b1-85ce26ee8662",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
